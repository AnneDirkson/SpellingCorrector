{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling correction \n",
    "\n",
    "--author -- AR Dirkson \n",
    "--date -- 5-2-2019 \n",
    "\n",
    "This script is a spelling correction module that uses unsupervised data to construct a list of candidates. The correction algorithm is a weighted Levenshtein distance algorithm. A decision process is used to determine if a word is a spelling mistake.\n",
    "\n",
    "It makes use of the CELEX generic dictionary but this can be substituted by another generic dictionary. It is only used to determine if a word should not be corrected because it is a generic word. \n",
    "\n",
    "The grid used for the spelling mistake detection was [0.05 - 0.15] (steps of 0.01) for relative weighted edit distance max and [2-10] (steps of 1) for relative corpus frequency multiplier. This can be re-tuned (tuning not included in this script).\n",
    "\n",
    "Note: the damlev module only works on Linux platforms and the input data needs to be tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import editdistance\n",
    "import re\n",
    "import csv \n",
    "import pandas as pd\n",
    "import pickle\n",
    "from weighted_levenshtein import lev, osa, dam_lev\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import scipy.stats \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellingCorrector(): \n",
    "    \n",
    "    def __init__(self): \n",
    "        pass\n",
    "    \n",
    "    def load_obj (self, path, name): \n",
    "        with open(path + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    def load_files (self): \n",
    "        #load the edit matrices\n",
    "        path = '/data/dirksonar/Project1_lexnorm/spelling_correction/output/'\n",
    "        #transpositions\n",
    "        self.edits_trans = self.load_obj(path, 'weighted_edits_transpositions')\n",
    "        #deletions \n",
    "        self.edits_del = self.load_obj(path,'weighted_edits_deletions')\n",
    "        #insertions \n",
    "        self.edits_ins = self.load_obj(path,'weighted_edits_insertions')\n",
    "        #substitutions\n",
    "        self.edits_sub = self.load_obj(path,'weighted_edits_substitutions')\n",
    "                \n",
    "        #load the generic dictionary - CHANGE PATH!  \n",
    "        self.celex_freq_dict = self.load_obj ('/home/dirksonar/Scripts/Project1_lexnorm/preprocessing_pipeline/obj_lex/', 'celex_lwrd_frequencies')\n",
    "    \n",
    "    \n",
    "    def initialize_weighted_matrices(self): \n",
    "    #initialize the cost matrixes for deletions and insertions\n",
    "        insert_costs = np.ones(128, dtype=np.float64)  # make an array of all 1's of size 128, the number of ASCII characters\n",
    "        delete_costs = np.ones (128, dtype=np.float64)\n",
    "\n",
    "        for index,row in self.edits_ins.iterrows(): \n",
    "            insert_costs[ord(index)] = row['transformed_frequency']\n",
    "\n",
    "        for index,row in self.edits_del.iterrows(): \n",
    "            delete_costs[ord(index)] = row['transformed_frequency']\n",
    "\n",
    "        #substitution\n",
    "\n",
    "        substitute_costs = np.ones((128, 128), dtype=np.float64)\n",
    "        lst = []\n",
    "        for index,row in self.edits_sub.iterrows(): \n",
    "            z = tuple([row['edit_from'], row['edit_to'], row['transformed_frequency']])\n",
    "            lst.append (z)\n",
    "        for itm in lst: \n",
    "            itm2 = list(itm)\n",
    "            try: \n",
    "                substitute_costs[ord(itm2[0]), ord(itm2[1])] = itm2[2]\n",
    "            except IndexError: \n",
    "                pass\n",
    "\n",
    "        #transposition\n",
    "\n",
    "        transpose_costs = np.ones((128, 128), dtype=np.float64)\n",
    "\n",
    "        lst = []\n",
    "\n",
    "        for index,row in self.edits_trans.iterrows(): \n",
    "            z = tuple([row['first_letter'], row['second_letter'], row['transformed_frequency']])\n",
    "            lst.append (z)\n",
    "\n",
    "        for itm in lst: \n",
    "            itm2 = list(itm)\n",
    "            try: \n",
    "                transpose_costs[ord(itm2[0]), ord(itm2[1])] = itm2[2]\n",
    "            except IndexError: \n",
    "                print(itm2)\n",
    "\n",
    "        return insert_costs, delete_costs, substitute_costs, transpose_costs\n",
    "\n",
    "    \n",
    "    def weighted_ed_rel (self, cand, token, del_costs, ins_costs, sub_costs, trans_costs): \n",
    "        w_editdist = dam_lev(token, cand, delete_costs = del_costs, insert_costs = ins_costs, \n",
    "                             substitute_costs = sub_costs, transpose_costs = trans_costs)\n",
    "        rel_w_editdist = w_editdist/len(token)\n",
    "        return rel_w_editdist\n",
    "\n",
    "    def run_low (self, word, voc, func, del_costs, ins_costs, sub_costs, trans_costs): \n",
    "        replacement = [' ',100]\n",
    "        for token in voc: \n",
    "            sim = func(word, token, del_costs, ins_costs, sub_costs, trans_costs)\n",
    "            if sim < replacement[1]:\n",
    "                replacement[1] = sim\n",
    "                replacement[0] = token\n",
    "\n",
    "        return replacement   \n",
    "    \n",
    "    \n",
    "    def spelling_correction (self, post, token_freq_dict, token_freq_ordered, min_rel_freq = 2, max_rel_edit_dist = 0.08): \n",
    "        post2 = []\n",
    "        cnt = 0 \n",
    "\n",
    "        for a, token in enumerate (post): \n",
    "            if self.TRUE_WORD.fullmatch(token):\n",
    "                if token in self.spelling_corrections:\n",
    "                    correct = self.spelling_corrections[token] \n",
    "                    post2.append(correct)\n",
    "                    cnt +=1\n",
    "                    self.replaced.append(token)\n",
    "                    self.replaced_with.append(correct)\n",
    "\n",
    "                elif token in self.celex_freq_dict:\n",
    "                    post2.append(token)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # make the subset of possible candidates\n",
    "                    freq_word = token_freq_dict[token]\n",
    "                    limit = freq_word * min_rel_freq\n",
    "                    subset = [t[0] for t in token_freq_ordered if t[1]>= limit]\n",
    "\n",
    "                    #compare these candidates with the word        \n",
    "                    candidate = self.run_low (token, subset, self.weighted_ed_rel, self.delete_costs_nw, self.insert_costs_nw, \n",
    "                                         self.substitute_costs_nw, self.transpose_costs_nw)\n",
    "\n",
    "                #if low enough RE - candidate is deemed good\n",
    "                    if candidate[1] <= max_rel_edit_dist:\n",
    "                        post2.append(candidate[0]) \n",
    "                        cnt +=1\n",
    "                        self.replaced.append(token)\n",
    "                        self.replaced_with.append(candidate[0])\n",
    "                        self.spelling_corrections [token] = candidate[0]\n",
    "                    else: \n",
    "                        post2.append(token)\n",
    "            else: post2.append(token)\n",
    "        self.total_cnt.append (cnt)\n",
    "        return post2\n",
    "      \n",
    "    def initialize_files_for_spelling(self): \n",
    "        total_cnt = []\n",
    "        replaced = []\n",
    "        replaced_with = []\n",
    "        spelling_corrections= {}\n",
    "        return total_cnt, replaced, replaced_with, spelling_corrections\n",
    "    \n",
    "    def change_tup_to_list (self, tup): \n",
    "        thelist = list(tup)\n",
    "        return thelist\n",
    "\n",
    "    def create_token_freq (self, data): \n",
    "        flat_data = [item for sublist in data for item in sublist]\n",
    "        self.token_freq = Counter(flat_data)\n",
    "        \n",
    "        token_freq_ordered = self.token_freq.most_common ()\n",
    "        self.token_freq_ordered2 = [self.change_tup_to_list(m) for m in token_freq_ordered]\n",
    "    \n",
    "    def correct_spelling_mistakes(self, data): \n",
    "#         data= self.load_obj ('/data/dirksonar/Project1_lexnorm/spelling_correction/output/', 'gistdata_lemmatised')\n",
    "        self.load_files()\n",
    "        self.insert_costs_nw, self.delete_costs_nw, self.substitute_costs_nw, self.transpose_costs_nw = self.initialize_weighted_matrices()\n",
    "        self.total_cnt, self.replaced, self.replaced_with, self.spelling_corrections = self.initialize_files_for_spelling()\n",
    "    \n",
    "        self.TRUE_WORD = re.compile('[-a-z]+')  # Only letters and dashes  \n",
    "#         data2 = [word_tokenize(m) for m in data]\n",
    "        self.create_token_freq(data)\n",
    "        out = [self.spelling_correction (m, self.token_freq, self.token_freq_ordered2) for m in data]\n",
    "        return out, self.total_cnt, self.replaced, self.replaced_with, self.spelling_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example script for running class \n",
    "out, total_cnt, replaced, replaced_with, spelling_corrections = SpellingCorrector().correct_spelling_mistakes(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9068\n"
     ]
    }
   ],
   "source": [
    "print(sum(total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gleevac', 344), ('metastatic', 277), ('gleevic', 215), ('gister', 134), ('mutational', 125), ('pas', 82), ('gon', 65), ('san', 53), ('listserve', 48), ('cosse', 45)]\n"
     ]
    }
   ],
   "source": [
    "c = Counter(replaced)\n",
    "print(c.most_common (10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
